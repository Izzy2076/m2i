# TP supplÃ©mentaire 1

## **Contexte Data Engineering**
Vous dÃ©veloppez un pipeline de donnÃ©es pour une startup qui analyse les performances de sites e-commerce. Le systÃ¨me doit traiter des logs d'API, des donnÃ©es de sessions utilisateurs, et des mÃ©triques business en utilisant uniquement Bash (orchestration) et Python (traitement) sur Linux.

## **ProblÃ©matique Data**
Le pipeline traite quotidiennement :
- **Logs d'API** : 200+ fichiers JSON (1-5GB chacun) avec requÃªtes utilisateurs
- **Sessions web** : CSV avec parcours clients, durÃ©es, conversions
- **MÃ©triques business** : Ventes, abandons panier, taux de rebond
- **DonnÃ©es produits** : Catalogues, prix, stock (Excel/CSV)
- **Log serveurs** : Nginx access logs avec gÃ©olocalisation

**DÃ©fi Data** : Traiter 50+ GB/jour avec parallÃ©lisation intelligente, transformations complexes, et gÃ©nÃ©ration de rapports business.

---

## **CONSIGNES TECHNIQUES PRÃ‰CISES**

### **ğŸ“‹ MISSION DATA ENGINEERING**
CrÃ©er un pipeline de donnÃ©es production-ready qui ingÃ¨re, transforme, et analyse automatiquement les donnÃ©es e-commerce avec Bash + Python.

### **MODULES OBLIGATOIRES**

#### **Module 1 : Data Orchestrator (Bash)** â­ **(PrioritÃ© Critique)**
```bash
#!/bin/bash
# CONSIGNE : Orchestrateur principal qui gÃ¨re le pipeline de donnÃ©es

# Architecture obligatoire :
data_pipeline_master.sh
â”œâ”€â”€ initialize_data_pipeline()   # Configuration environnement data
â”œâ”€â”€ scan_data_sources()          # DÃ©couverte automatique des sources
â”œâ”€â”€ distribute_processing()      # RÃ©partition par type/taille de donnÃ©es  
â”œâ”€â”€ monitor_data_quality()       # Surveillance qualitÃ© en temps rÃ©el
â”œâ”€â”€ aggregate_data_results()     # Consolidation multi-sources
â”œâ”€â”€ generate_data_reports()      # Rapports automatiques
â””â”€â”€ archive_processed_data()     # Archivage avec compression

# Variables data obligatoires :
DATA_WORKERS=6               # Workers spÃ©cialisÃ©s par type de donnÃ©es
CHUNK_SIZE_MB=500           # Taille des chunks de traitement
QUALITY_THRESHOLD=95        # Seuil minimal de qualitÃ© (%)
PROCESSING_TIMEOUT=3600     # Timeout par fichier (1h)
```

**FonctionnalitÃ©s Bash data-specific :**
- ğŸ“ **Data discovery** : Auto-dÃ©tection de formats (CSV, JSON, logs)
- ğŸ”„ **Pipeline restart** : Reprise automatique aprÃ¨s crash
- ğŸ’¾ **Data partitioning** : Organisation par date/source/format

#### **Module 2 : Data Processor (Python)** â­ **(PrioritÃ© Critique)**
```python
#!/usr/bin/env python3
# CONSIGNE : Moteur de traitement de donnÃ©es multi-format


```

**Transformations data obligatoires :**
- ğŸ§¹ **Data cleaning** : DÃ©duplication, gestion des nulls, normalisation
- ğŸ”— **Data joining** : Jointures complexes entre sources hÃ©tÃ©rogÃ¨nes
- ğŸ“ˆ **Data aggregation** : GroupBy, pivots, window functions
- ğŸ¯ **Data enrichment** : GÃ©ocodage, catÃ©gorisation, scoring
- âš¡ **Data validation** : RÃ¨gles mÃ©tier, contraintes d'intÃ©gritÃ©

#### **Module 3 : Data Quality Manager (Bash + Python)** â­ **(PrioritÃ© Haute)**
```bash
#!/bin/bash
# CONSIGNE : Gestionnaire de qualitÃ© des donnÃ©es

data_quality_monitor.sh() {
    # OBLIGATOIRE : Validation de schÃ©mas (colonnes attendues/reÃ§ues)
    # OBLIGATOIRE : ContrÃ´les de cohÃ©rence inter-fichiers
    # OBLIGATOIRE : DÃ©tection d'anomalies statistiques
    # OBLIGATOIRE : Alertes automatiques si qualitÃ© < seuil
    
    
}
```

**ContrÃ´les qualitÃ© data obligatoires :**
- ğŸ“‹ **Schema validation** : VÃ©rification structure attendue
- ğŸš¨ **Anomaly detection** : Valeurs aberrantes, pics suspects
- ğŸ¯ **Business rules** : Validation des rÃ¨gles mÃ©tier

### **ğŸ”§ ARCHITECTURE DATA**

#### **Structure data pipeline obligatoire :**


```

### **ğŸ“Š EXIGENCES DATA ENGINEERING**

#### **Performance data obligatoire :**
- ğŸ”„ **Parallelism** : 6 workers Python simultanÃ©s
- ğŸ’¾ **Memory efficiency** : Traitement par chunks de 100K lignes

#### **FonctionnalitÃ©s data avancÃ©es :**
```python
# EXEMPLES D'IMPLÃ‰MENTATIONS OBLIGATOIRES

def incremental_processing(data_dir, last_processed_timestamp):
    """Traitement incrÃ©mental - ne traite que les nouvelles donnÃ©es"""
    # Delta processing pour Ã©viter de retraiter tout l'historique
    pass

def data_lineage_tracker(input_files, transformations, output_files):
    """TraÃ§abilitÃ© complÃ¨te des transformations de donnÃ©es"""
    # Tracking de chaque Ã©tape pour audit et debug
    pass

def adaptive_chunking(file_size, available_memory):
    """Calcul automatique de la taille des chunks selon les ressources"""
    # Optimisation dynamique selon la mÃ©moire disponible
    pass

def data_quality_scoring(dataframe, quality_rules):
    """Calcul de score de qualitÃ© multi-dimensionnel"""
    # Score composite : complÃ©tude, exactitude, cohÃ©rence, fraÃ®cheur
    pass
```

### **ğŸ¯ LIVRABLES DATA ENGINEERING**

#### **1. Pipeline Data Fonctionnel** 

#### **2. Code Data Production** 

#### **3. Documentation Data** 
---

